{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom cnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env1/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appunti - Custom Estimator\n",
    "```python\n",
    "# input will be: [[1,2,3],[2,10,3],...] and this layer will \n",
    "# output [[0.2,0.3...], len([...]) = EMBEDDING_SIZE]\n",
    "tf.contrib.layers.embed_sequence(  \n",
    "    ids: # [batch_size, doc_length] Tensor of type int32 or int64 with symbol ids.  \n",
    "    vocab_size: # Integer number of symbols in vocabulary.  \n",
    "    embed_dim: # Integer number of dimensions for embedding matrix.  \n",
    "    initializer: # An initializer for the embeddings, if None default for current scope is used.  \n",
    "    )  \n",
    "```\n",
    "-----------------------------\n",
    "```python\n",
    "# finestra di lunghezza kernel_size che si sposta all'interno di ogni vettore in input\n",
    "conv = tf.layers.conv1d(\n",
    "    inputs= # Tensor input\n",
    "    filters= # the dimensionality of the output space (i.e. the number of filters in the convolution).\n",
    "    kernel_size= # An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "    padding= # One of \"valid\" or \"same\" (case-insensitive).\n",
    "    activation= # function. Set it to None to maintain a linear activation.\n",
    ")\n",
    "```\n",
    "--------------------\n",
    "```python\n",
    "# riduco la dimensione dell'uscita di conv prendendo il valore max per ogni vettore\n",
    "pool = tf.reduce_max(input_tensor=conv, axis=1) \n",
    "```\n",
    "--------------------\n",
    "```python\n",
    "# ottengo un tensore con un label per riga, tutto su una sola colonna\n",
    "# my alternative: labels = tf.reshape(labels, [-1, 1]) if mode != tf.estimator.ModeKeys.PREDICT else labels\n",
    "labels = tf.reshape(labels, [-1, 1])\n",
    "```\n",
    "----------------------\n",
    "### head\n",
    "```python\n",
    "# Given logits (or output of a hidden layer), a Head knows how to compute predictions, loss, default metric and export signature\n",
    "head = tf.contrib.estimator.binary_classification_head() # Creates a _Head for single label binary classification\n",
    "output = head.create_estimator_spec(\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    mode=mode,\n",
    "    logits=logits, \n",
    "    train_op_fn=_train_op_fn)\n",
    "```\n",
    "------------------------\n",
    "### my_model_fn(features, labels, mode)\n",
    "```python\n",
    "def my_model_fn(\n",
    "   features, # This is batch_features from input_fn\n",
    "   labels,   # This is batch_labels from input_fn\n",
    "   mode,     # Instance of tf.estimator.ModeKeys, see below\n",
    "   params):  # {} facoltativo\n",
    "    \n",
    "# Se uso params allora alla creazione\n",
    "# params = {...}\n",
    "# my_class = tf.estimator.Estimator(model_fn=my_model_fn,\n",
    "#                                         model_dir=...,\n",
    "#                                         params=params)\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1,2,3],[4,5,6]] # frase con parole sostituite da indici\n",
    "\n",
    "vocabulary = {'cat':1, 'is':2} # vocabolario con tutte le parole e i relativi indici\n",
    "\n",
    "EMBEDDING_SIZE = 10 # 'cat' -> len(embedding('cat'))\n",
    "\n",
    "head = tf.contrib.estimator.binary_classification_head() # Creates a _Head for single label binary classification\n",
    "\n",
    "# Creates a _Head for single label binary classification\n",
    "head = tf.contrib.estimator.binary_classification_head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    \n",
    "    # INPUT\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], len(vocabulary), EMBEDDING_SIZE,   # posso fare features['x'] perch√® prima ho fatto dataset = dataset.map(parser)\n",
    "        initializer=params['embedding_initializer'])\n",
    "     \n",
    "    # DROPOUT LAYER\n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "                                    rate=0.2, \n",
    "                                    training=training) # se non sono in training niente dropout\n",
    "    \n",
    "    \n",
    "    # CNN\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=dropout_emb,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    \n",
    "    \n",
    "    # Full connected layer + dropout\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "                                       rate=0.2, \n",
    "                                       training=training)\n",
    "    \n",
    "    # Output-logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    \n",
    "    # Metto il label in (una) colonna\n",
    "    if labels is not None: # This will be None when predicting\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        \n",
    "    \n",
    "    # Gradient descend optimizator\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    \n",
    "    # Usata nell'output\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    \n",
    "    \n",
    "    # Output\n",
    "    # head = tf.contrib.estimator.binary_classification_head()\n",
    "    output = head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
