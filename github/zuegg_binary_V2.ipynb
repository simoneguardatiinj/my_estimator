{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom estimator - binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source env1/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorboard import summary as summary_lib\n",
    "from tensorflow.python.lib.io.file_io import FileIO as open_file\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants of the programm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'model_dir_v2'\n",
    "\n",
    "INPUT_FILE_CSV = 'gs://guardati-test/comments/raw_Opinioni_Zuegg_LinkMultipli_campi_standard.csv'\n",
    "N_COMMENTS_TO_IMPORT = 1000\n",
    "\n",
    "WORDS_NOT_ALLOWED = [',','.',':',';']\n",
    "INDEX_PADDING = 0\n",
    "\n",
    "SENTENCE_LEN = 30 # NÂ° of words per sentences\n",
    "\n",
    "TRAIN_QUOTE = 0.6\n",
    "\n",
    "TRAIN_STEPS = 10\n",
    "\n",
    "EMBEDDING_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive the csv with the comments\n",
    "- the comment must to be under a column named 'Testo' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open_file(INPUT_FILE_CSV,'r') as f:\n",
    "    df=pd.read_csv(f,skip_blank_lines=True)\n",
    "    df = df['Testo'][0:N_COMMENTS_TO_IMPORT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary creation\n",
    "- words_dict = {'word', word_unique_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len(words_dict) = ', 9450)\n"
     ]
    }
   ],
   "source": [
    "index = INDEX_PADDING + 1\n",
    "words_dict = {}\n",
    "words_dict_inverted = {}\n",
    "comments_splitted = []\n",
    "\n",
    "for comment in df:\n",
    "    for word_not_allowed in WORDS_NOT_ALLOWED:\n",
    "        comment = comment.replace(word_not_allowed, ' ')\n",
    "    \n",
    "    words = comment.lower().split()\n",
    "    \n",
    "    comments_splitted.append(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in words_dict:\n",
    "            words_dict[word] = index\n",
    "            words_dict_inverted[index] = word\n",
    "            index += 1\n",
    "\n",
    "print('len(words_dict) = ',len(words_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert comments -> list[word1_idx, word2_idx...wordN_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_int = []\n",
    "\n",
    "start = True   # maybe exist a more fashion way\n",
    "\n",
    "for comment in comments_splitted:\n",
    "    comment_int = []\n",
    "    for word in comment:\n",
    "        comment_int.append(words_dict[word])\n",
    "        \n",
    "    if start:\n",
    "        comments_int = [comment_int[0:SENTENCE_LEN]]\n",
    "        start = False\n",
    "    else:    \n",
    "        comments_int = np.append(comments_int,\n",
    "                            [comment_int[0:SENTENCE_LEN]],axis=0)\n",
    "    \n",
    "comments_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_inverter(comment):\n",
    "    txt = ''\n",
    "    for word in comment:\n",
    "        txt += ' '\n",
    "        txt += words_dict_inverted[word]\n",
    "    txt += '\\n'\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels creation - binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed=10)\n",
    "np.set_printoptions(1000)\n",
    "\n",
    "x_data = comments_int\n",
    "y_data = np.random.randint(2,size=(len(x_data),1))\n",
    "\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (600, 30), (600, 1))\n",
      "('Test: ', (400, 30), (400, 1))\n"
     ]
    }
   ],
   "source": [
    "len_data = len(x_data)\n",
    "\n",
    "x_train = x_data[:int(round(len_data * TRAIN_QUOTE))]\n",
    "y_train = y_data[:int(round(len_data * TRAIN_QUOTE))]\n",
    "\n",
    "x_test = x_data[len(x_train):]\n",
    "y_test = y_data[len(y_train):]\n",
    "\n",
    "print('Train: ',x_train.shape,y_train.shape)\n",
    "print('Test: ',x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "\n",
    "### Input tensor making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x, y):\n",
    "    features = {'x': x}\n",
    "    return features, y\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size = len(x_data))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    dataset = dataset.batch(100)\n",
    "    dataset = dataset.map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator tester function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "def train_and_evaluate(classifier):\n",
    "    # Save a reference to the classifier to run predictions later\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=TRAIN_STEPS)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "        \n",
    "    # Reset the graph to be able to reuse name scopes\n",
    "    tf.reset_default_graph() \n",
    "    # Add a PR summary in addition to the summaries that the classifier writes\n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canned linear estimator - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbffc57a810>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'model_dir_v2/bow_sparse', '_train_distribute': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "column = tf.feature_column.categorical_column_with_identity('x', len(words_dict))\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=[column], model_dir=os.path.join(MODEL_DIR, 'bow_sparse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_dir_v2/bow_sparse/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 30 into model_dir_v2/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.934425, step = 31\n",
      "INFO:tensorflow:Saving checkpoints for 40 into model_dir_v2/bow_sparse/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.727288.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-25-15:23:33\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_dir_v2/bow_sparse/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-25-15:23:34\n",
      "INFO:tensorflow:Saving dict for global step 40: accuracy = 0.51, accuracy_baseline = 0.52250004, auc = 0.5280443, auc_precision_recall = 0.4701691, average_loss = 0.87123793, global_step = 40, label/mean = 0.4775, loss = 87.123795, precision = 0.4863388, prediction/mean = 0.48179764, recall = 0.46596858\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: model_dir_v2/bow_sparse/model.ckpt-40\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_dir_v2/bow_sparse/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(classifier)\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = tf.contrib.estimator.binary_classification_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):    \n",
    "    \n",
    "    # INPUT\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], len(words_dict), EMBEDDING_SIZE,\n",
    "        initializer=params['embedding_initializer'])\n",
    "    \n",
    "    # DROPOUT LAYER\n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer, \n",
    "                                    rate=0.2, \n",
    "                                    training=training)\n",
    "    \n",
    "    # CNN\n",
    "    conv = tf.layers.conv1d(\n",
    "        inputs=dropout_emb,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Global Max Pooling\n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    \n",
    "    # Full connected layer + dropout\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, \n",
    "                                       rate=0.2, \n",
    "                                       training=training)\n",
    "    \n",
    "    # Output-logits layer\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    # Metto il label in (una) colonna\n",
    "    if labels is not None: \n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        \n",
    "    # Gradient descend optimizator\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    # Usata nell'output\n",
    "    def _train_op_fn(loss):\n",
    "        return optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Output\n",
    "    # head = tf.contrib.estimator.binary_classification_head()\n",
    "    output = head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits, \n",
    "        train_op_fn=_train_op_fn)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding (farlocco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc001682e50>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'model_dir_v2/cnn', '_train_distribute': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(MODEL_DIR, 'cnn'),\n",
    "                                        params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into model_dir_v2/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.72143805, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into model_dir_v2/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7081385.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-25-15:33:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_dir_v2/cnn/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-25-15:33:15\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.495, accuracy_baseline = 0.52250004, auc = 0.51109743, auc_precision_recall = 0.47006035, average_loss = 0.6926444, global_step = 10, label/mean = 0.4775, loss = 0.6926444, precision = 0.42857143, prediction/mean = 0.47903916, recall = 0.17277487\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: model_dir_v2/cnn/model.ckpt-10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_dir_v2/cnn/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(cnn_classifier)\n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
